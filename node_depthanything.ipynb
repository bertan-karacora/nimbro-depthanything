{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ros2 topic info /camera_ids/image_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "image_cv = np.ones((1920, 2556, 3), dtype=np.uint8) * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(2556 / 1920 * 518 / 14) * 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 518, 686])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a690bb700>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as tv_transforms\n",
    "import cv2\n",
    "\n",
    "import nimbro_depthanything.models.util.transform as custom_transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = 518\n",
    "\n",
    "transform = tv_transforms.Compose(\n",
    "    [\n",
    "        custom_transforms.Resize(\n",
    "            width=input_size,\n",
    "            height=input_size,\n",
    "            resize_target=False,\n",
    "            keep_aspect_ratio=True,\n",
    "            ensure_multiple_of=14,\n",
    "            resize_method=\"lower_bound\",\n",
    "            image_interpolation_method=cv2.INTER_CUBIC,\n",
    "        ),\n",
    "        custom_transforms.NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        custom_transforms.PrepareForNet(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB) / 255.0\n",
    "image = transform({\"image\": image})[\"image\"]\n",
    "image = torch.from_numpy(image)[None, ...]\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.imshow(image[0].permute((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model ...\n",
      "Model exported to /root/colcon_ws/src/nimbro_depthanything/resources/models_onnx/depth_anything_v2_metric_hypersim_vits.onnx\n"
     ]
    }
   ],
   "source": [
    "import nimbro_depthanything.scripts.export_model as export_model\n",
    "\n",
    "export_model.export_model(\"depth_anything_v2_metric_hypersim_vits\", shape_input=(1, 3, 686, 686))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tensorrt engine ...\n",
      "[09/16/2024-12:54:56] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 776, GPU 15467 (MiB)\n",
      "[09/16/2024-12:54:58] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2235, GPU +426, now: CPU 3166, GPU 15893 (MiB)\n",
      "[09/16/2024-12:54:58] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/16/2024-12:54:58] [TRT] [I] Input filename:   /root/colcon_ws/src/nimbro_depthanything/resources/models_onnx/depth_anything_v2_metric_hypersim_vits.onnx\n",
      "[09/16/2024-12:54:58] [TRT] [I] ONNX IR version:  0.0.8\n",
      "[09/16/2024-12:54:58] [TRT] [I] Opset version:    17\n",
      "[09/16/2024-12:54:58] [TRT] [I] Producer name:    pytorch\n",
      "[09/16/2024-12:54:58] [TRT] [I] Producer version: 2.3.0\n",
      "[09/16/2024-12:54:58] [TRT] [I] Domain:           \n",
      "[09/16/2024-12:54:58] [TRT] [I] Model version:    0\n",
      "[09/16/2024-12:54:58] [TRT] [I] Doc string:       \n",
      "[09/16/2024-12:54:58] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/16/2024-12:54:58] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/16/2024-12:54:59] [TRT] [I] Compiler backend is used during engine build.\n",
      "[09/16/2024-12:55:55] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[09/16/2024-12:55:56] [TRT] [I] Total Host Persistent Memory: 189472 bytes\n",
      "[09/16/2024-12:55:56] [TRT] [I] Total Device Persistent Memory: 150016 bytes\n",
      "[09/16/2024-12:55:56] [TRT] [I] Max Scratch Memory: 11145216 bytes\n",
      "[09/16/2024-12:55:56] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 63 steps to complete.\n",
      "[09/16/2024-12:55:56] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.741568ms to assign 7 blocks to 63 nodes requiring 57089024 bytes.\n",
      "[09/16/2024-12:55:56] [TRT] [I] Total Activation Memory: 57089024 bytes\n",
      "[09/16/2024-12:55:56] [TRT] [I] Total Weights Memory: 50637856 bytes\n",
      "[09/16/2024-12:55:56] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[09/16/2024-12:55:56] [TRT] [I] Engine generation completed in 57.7948 seconds.\n",
      "[09/16/2024-12:55:56] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 40 MiB, GPU 269 MiB\n",
      "[09/16/2024-12:55:56] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 5506 MiB\n",
      "Engine saved to /root/colcon_ws/src/nimbro_depthanything/resources/engines/depth_anything_v2_metric_hypersim_vits.engine\n"
     ]
    }
   ],
   "source": [
    "import nimbro_depthanything.scripts.build_engine as build_engine\n",
    "\n",
    "build_engine.build_engine(\"depth_anything_v2_metric_hypersim_vits\", shape_input=(1, 3, 686, 686))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from cv_bridge import CvBridge\n",
    "from message_filters import ApproximateTimeSynchronizer, Cache, Subscriber as SubscriberFilter\n",
    "from rclpy.callback_groups import MutuallyExclusiveCallbackGroup, ReentrantCallbackGroup\n",
    "from rclpy.duration import Duration\n",
    "from rclpy.time import Time\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import HistoryPolicy, ReliabilityPolicy, QoSProfile\n",
    "from rcl_interfaces.msg import FloatingPointRange, IntegerRange, ParameterDescriptor, ParameterType\n",
    "from sensor_msgs.msg import CameraInfo, Image, PointCloud2, PointField\n",
    "from std_msgs.msg import Header\n",
    "from tf2_ros import TransformBroadcaster\n",
    "from tf2_ros.buffer import Buffer\n",
    "from tf2_ros.transform_listener import TransformListener\n",
    "\n",
    "import nimbro_utils.compat.point_cloud2 as point_cloud2\n",
    "from nimbro_utils.parameter_handler import ParameterHandler\n",
    "from nimbro_utils.tf_oracle import TFOracle\n",
    "\n",
    "import time\n",
    "\n",
    "from nimbro_depthanything.metric_depth.depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "\n",
    "class NodeDepthAnything(Node):\n",
    "    def __init__(\n",
    "        self,\n",
    "        topic_image=\"/camera_ids/image_color\",\n",
    "        topic_inferred_depth=\"/camera_ids/inferred/depth/image\",\n",
    "        name_encoder=\"small\",\n",
    "        name_dataset=\"hypersim\",\n",
    "        max_depth=20,\n",
    "    ):\n",
    "        super().__init__(node_name=\"depth_anything\")\n",
    "\n",
    "        self.bridge_cv = None\n",
    "        self.device = None\n",
    "        self.name_dataset = name_dataset\n",
    "        self.name_encoder = name_encoder\n",
    "        self.handler_parameters = None\n",
    "        self.max_depth = max_depth\n",
    "        self.profile_qos = None\n",
    "        self.service_colorize_points = None\n",
    "        self.service_project_dome = None\n",
    "        self.subscriber_image = None\n",
    "        self.topic_image = topic_image\n",
    "        self.topic_inferred_depth = topic_inferred_depth\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bridge_cv = CvBridge()\n",
    "        self.profile_qos = QoSProfile(reliability=ReliabilityPolicy.BEST_EFFORT, history=HistoryPolicy.KEEP_LAST, depth=1)\n",
    "        self.handler_parameters = ParameterHandler(self, verbose=False)\n",
    "\n",
    "        # self._init_parameters()\n",
    "\n",
    "        # self._init_tf_oracle()\n",
    "        # self._del_publishers()\n",
    "\n",
    "        # self._del_services()\n",
    "        # self._init_services()\n",
    "        # self._del_subscribers()\n",
    "\n",
    "        model_configs = {\n",
    "            \"small\": {\"encoder\": \"vits\", \"features\": 64, \"out_channels\": [48, 96, 192, 384]},\n",
    "            \"base\": {\"encoder\": \"vitb\", \"features\": 128, \"out_channels\": [96, 192, 384, 768]},\n",
    "            \"large\": {\"encoder\": \"vitl\", \"features\": 256, \"out_channels\": [256, 512, 1024, 1024]},\n",
    "        }\n",
    "\n",
    "        self.model = DepthAnythingV2(**{**model_configs[self.name_encoder], \"max_depth\": self.max_depth})\n",
    "        name_encoder_internal = model_configs[self.name_encoder][\"encoder\"]\n",
    "        self.model.load_state_dict(torch.load(f\"checkpoints/depth_anything_v2_metric_{self.name_dataset}_{name_encoder_internal}.pth\"))\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.i = 0\n",
    "\n",
    "        self._init_subscribers()\n",
    "        self._init_publishers()\n",
    "\n",
    "    def _init_subscribers(self):\n",
    "        self.subscriber_image = self.create_subscription(Image, self.topic_image, self.infer_depth, qos_profile=self.profile_qos, callback_group=MutuallyExclusiveCallbackGroup())\n",
    "\n",
    "    def _init_publishers(self):\n",
    "        self.publisher_image = self.create_publisher(msg_type=Image, topic=self.topic_inferred_depth, qos_profile=self.profile_qos, callback_group=ReentrantCallbackGroup())\n",
    "\n",
    "    def publish_image(self, image, name_frame, stamp):\n",
    "        header = Header(stamp=stamp, frame_id=name_frame)\n",
    "        message = self.bridge_cv.cv2_to_imgmsg(image, header=header, encoding=\"mono16\")\n",
    "\n",
    "        self.publisher_depth.publish(message)\n",
    "\n",
    "    def visualize_depth_image(self, image_depth, image_rgb, time_needed):\n",
    "        din_a4 = np.array([210, 297]) / 25.4\n",
    "        din_a4_landscape = din_a4[::-1]\n",
    "        fig = plt.figure(figsize=din_a4_landscape)\n",
    "\n",
    "        def visualize_image(image):\n",
    "            ax = plt.gca()\n",
    "            ax.set_axis_off()\n",
    "            ax.imshow(image, cmap=\"turbo\", vmin=0)\n",
    "\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        visualize_image(image_rgb)\n",
    "\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        visualize_image(image_depth)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"Inference time: {time_needed:.4f}\")\n",
    "        # plt.savefig(Path(\".\") / \"gif\" / f\"image_{self.i}\")\n",
    "        # self.i += 1\n",
    "        plt.show()\n",
    "\n",
    "    def infer_depth(self, message_image):\n",
    "        image = self.bridge_cv.imgmsg_to_cv2(message_image, desired_encoding=\"passthrough\")\n",
    "        # image = torch.as_tensor(image, dtype=torch.float16, device=self.device)\n",
    "\n",
    "        s = time.time()\n",
    "        image_depth = self.model.infer_image(image)  # HxW depth map in meters in numpy\n",
    "        e = time.time()\n",
    "\n",
    "        self.get_logger().info(f\"Inference time: {e-s}\")\n",
    "\n",
    "        self.get_logger().info(f\"Mean depth: {image_depth.mean()}\")\n",
    "        self.get_logger().info(f\"Shape: {image_depth.shape}\")\n",
    "\n",
    "        # self.publish_image(image_depth, name_frame=message_image.header.frame_id, stamp=message_image.header.stamp)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        time_needed = e - s\n",
    "        self.visualize_depth_image(image_depth, image, time_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nimbro_utils.node as utils_node\n",
    "import rclpy\n",
    "\n",
    "try:\n",
    "    rclpy.shutdown()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "utils_node.start_and_spin_node(NodeDepthAnything)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
